{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "import commonfunctions as cf # this a custom module found the commonfunctions.py\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.draw import polygon\n",
    "from skimage.color import rgb2gray,rgb2lab\n",
    "from scipy import fftpack\n",
    "from scipy.signal import convolve2d\n",
    "from skimage.util import random_noise\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.morphology import binary_erosion\n",
    "from skimage.morphology import disk,square,rectangle\n",
    "from skimage.transform import PiecewiseAffineTransform\n",
    "from skimage.transform import warp\n",
    "import skimage\n",
    "from scipy.sparse import spdiags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "# Show the matlpotlib figures inside the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type tuple)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-70b40f592da1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0my2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbottom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbottom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mlandmarks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: an integer is required (got type tuple)"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = detector(gray)\n",
    "    for face in faces:\n",
    "        x1 = face.left()+(face.left()*0.2)\n",
    "        y1 = face.top()+(face.top()*1.1)\n",
    "        x2 = face.right()+(face.right()*0.2)\n",
    "        y2 = face.bottom()+(face.bottom()*0.1)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        for n in range(0, 66):\n",
    "            x = landmarks.part(n).x\n",
    "            y = landmarks.part(n).y\n",
    "            cv2.circle(frame, (x, y), 4, (255, 0, 0), -1)\n",
    "    \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face_cascade = cv2.CascadeClassifier('C:\\\\Users\\\\bia3\\\\Anaconda3\\\\Lib\\\\site-packages\\\\cv2\\\\data\\\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "# img = cv2.imread('70289503_2447351508705408_6307667250979536896_o.jpg')\n",
    "# gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "# for (x,y,w,h) in faces:\n",
    "# #     right=int((x+w)+((x+w)*0.2))\n",
    "# #     left=int(x+(x*0.2))\n",
    "# #     top=int((y+h)+((y+h)*1.1))\n",
    "# #     bottom=int(y+(y*0.1))\n",
    "#     right=int(x+w)\n",
    "#     left=int(x)\n",
    "#     top=int((y+h))\n",
    "#     bottom=int(y)\n",
    "#     print(right,left,top,bottom)\n",
    "#     img = cv2.rectangle(img,(left,bottom),(right,top),(255,0,0),2)\n",
    "#     roi_gray = gray[bottom:top, left:right]\n",
    "#     roi_color = img[bottom:top, left:right]\n",
    "\n",
    "# cv2.imshow('img',img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # set up the 68 point facial landmark detector\n",
    "# detector = dlib.get_frontal_face_detector()\n",
    "# predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "# # bring in the input image\n",
    "# img = cv2.imread('70289503_2447351508705408_6307667250979536896_o.jpg', 1)\n",
    "\n",
    "# # convert to grayscale\n",
    "# img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # detect faces in the image\n",
    "# faces_in_image = detector(img_gray, 0)\n",
    "# # loop through each face in image\n",
    "# for face in faces_in_image:\n",
    "\n",
    "#     # assign the facial landmarks\n",
    "#     landmarks = predictor(img_gray, face)\n",
    "\n",
    "#     # unpack the 68 landmark coordinates from the dlib object into a list \n",
    "#     landmarks_list = []\n",
    "#     for i in range(0, landmarks.num_parts):\n",
    "#         landmarks_list.append((landmarks.part(i).x, landmarks.part(i).y))\n",
    "\n",
    "#     # for each landmark, plot and write number\n",
    "#     for landmark_num, xy in enumerate(landmarks_list, start = 1):\n",
    "#         cv2.circle(img, (xy[0], xy[1]), 12, (168, 0, 20), -1)\n",
    "#         cv2.putText(img, str(landmark_num),(xy[0]-7,xy[1]+5), cv2.FONT_HERSHEY_SIMPLEX, 0.4,(255,255,255), 1)\n",
    "\n",
    "\n",
    "# pb=(landmarks.part(8).x,landmarks.part(8).y)\n",
    "# pt=(landmarks.part(28).x,landmarks.part(28).y)\n",
    "# pl=(landmarks.part(17).x,landmarks.part(17).y)\n",
    "# pr=(landmarks.part(26).x,landmarks.part(26).y)\n",
    "\n",
    "# image = cv2.imread('70289503_2447351508705408_6307667250979536896_o.jpg', 1)\n",
    "\n",
    "# # top=int(pt[1]+pt[1]*1.1)\n",
    "# # bottom=int(pb[1]+pb[1]*0.1)\n",
    "# # left=int(pl[0]+pl[0]*0.2)\n",
    "# # right=int(pr[0]+pr[0]*0.2)\n",
    "\n",
    "# image = img\n",
    "# r = np.array([landmarks.part(8).y,landmarks.part(28).y,landmarks.part(17).y,landmarks.part(26).y])\n",
    "# c = np.array([landmarks.part(8).x,landmarks.part(28).x,landmarks.part(17).x,landmarks.part(26).x])\n",
    "\n",
    "# rr, cc = polygon(r, c)\n",
    "# img[rr, cc, 1] = 1\n",
    "\n",
    "# left=int(pl[0]-pl[0]*0.1)\n",
    "# right=pr[0]\n",
    "# bottom=int(pb[1]+pb[1]*0.1)\n",
    "# top=int(pt[1]-pt[1]*0.8)\n",
    "\n",
    "# img = cv2.rectangle(img,(left,bottom),(right,top),(255,0,0),2)\n",
    "\n",
    "# io.imshow(img)\n",
    "# io.show()\n",
    "\n",
    "# # for j in range(image.shape[1]-1):\n",
    "# #         for i in range(image.shape[0]-1):\n",
    "# #             if((i in range(landmarks.part(16).y,landmarks.part(3).y)) and (j in range(landmarks.part(3).x,landmarks.part(13).x))):\n",
    "# #                 image[i,j]=255\n",
    "# #             else:\n",
    "# #                 image[i,j]=0\n",
    "\n",
    "# # 9,29,27,18\n",
    "# # visualise the image with landmarks\n",
    "\n",
    "\n",
    "# io.imshow(image)  \n",
    "# io.show()\n",
    "\n",
    "# cv2.imshow('img',img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.0) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9e90f1ac07e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# convert to grayscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mimg_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# detect faces in the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.0) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "# set up the 68 point facial landmark detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "# bring in the input image\n",
    "img = cv2.imread('70289503_2447351508705408_6307667250979536896_o.jpg', 1)\n",
    "\n",
    "# convert to grayscale\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# detect faces in the image\n",
    "faces_in_image = detector(img_gray, 0)\n",
    "# loop through each face in image\n",
    "for face in faces_in_image:\n",
    "\n",
    "    # assign the facial landmarks\n",
    "    landmarks = predictor(img_gray, face)\n",
    "\n",
    "    # unpack the 68 landmark coordinates from the dlib object into a list \n",
    "    landmarks_list = []\n",
    "    for i in range(0, landmarks.num_parts):\n",
    "        landmarks_list.append((landmarks.part(i).x, landmarks.part(i).y))\n",
    "\n",
    "    # for each landmark, plot and write number\n",
    "    for landmark_num, xy in enumerate(landmarks_list, start = 1):\n",
    "        cv2.circle(img, (xy[0], xy[1]), 12, (168, 0, 20), -1)\n",
    "        cv2.putText(img, str(landmark_num),(xy[0]-7,xy[1]+5), cv2.FONT_HERSHEY_SIMPLEX, 0.4,(255,255,255), 1)\n",
    "print(landmarks_list)        \n",
    "cv2.imshow('img',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the 68 point facial landmark detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "# bring in the input image\n",
    "# img = cv2.imread('maxresdefault.jpg', 1)\n",
    "img = cv2.imread('elip.jpg', 1)\n",
    "# convert to grayscale\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# detect faces in the image\n",
    "faces_in_image = detector(img_gray, 0)\n",
    "# loop through each face in image\n",
    "for face in faces_in_image:\n",
    "\n",
    "    # assign the facial landmarks\n",
    "    landmarksref = predictor(img_gray, face)\n",
    "\n",
    "    # unpack the 68 landmark coordinates from the dlib object into a list \n",
    "    landmarks_listref = []\n",
    "    for i in range(0, landmarks.num_parts):\n",
    "        landmarks_listref.append((landmarksref.part(i).x, landmarksref.part(i).y))\n",
    "\n",
    "    # for each landmark, plot and write number\n",
    "    for landmark_numref, xy in enumerate(landmarks_listref, start = 1):\n",
    "        cv2.circle(img, (xy[0], xy[1]), 12, (168, 0, 20), -1)\n",
    "        cv2.putText(img, str(landmark_numref),(xy[0]-7,xy[1]+5), cv2.FONT_HERSHEY_SIMPLEX, 0.4,(255,255,255), 1)\n",
    "        \n",
    "cv2.imshow('img',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bia3\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Non-string object detected for the array ordering. Please pass in 'C', 'F', 'A', or 'K' instead\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\bia3\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: DeprecationWarning: Non-string object detected for the array ordering. Please pass in 'C', 'F', 'A', or 'K' instead\n",
      "C:\\Users\\bia3\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: DeprecationWarning: Non-string object detected for the array ordering. Please pass in 'C', 'F', 'A', or 'K' instead\n"
     ]
    }
   ],
   "source": [
    "imgr = cv2.imread('elip.jpg', 1)\n",
    "imgt = cv2.imread('bastf.jpg', 1)\n",
    "\n",
    "imager = rgb2lab(imgr)\n",
    "imaget = rgb2lab(imgt)\n",
    "\n",
    "def wlsfilter_layer(image_orig, beta=0.2 ,lambda_=0.2):\n",
    "\n",
    "    image = image_orig.astype(np.float)/255.0\n",
    "    image1 = image.flatten(1)\n",
    "    s = image.shape\n",
    "    k = s[0]*s[1]\n",
    "\n",
    "    dy = np.diff(image, 1, 0)\n",
    "    dx = np.diff(image, 1, 1)\n",
    "    \n",
    "    dy = -beta*lambda_ / ((np.absolute(dy) ** 1.2 + 0.0001))\n",
    "    dx = -beta*lambda_ / ((np.absolute(dx) ** 1.2 + 0.0001))\n",
    "\n",
    "\n",
    "    dy = np.vstack((dy, np.zeros(s[1], )))\n",
    "    dx = np.hstack((dx, np.zeros(s[0], )[:, np.newaxis]))\n",
    "\n",
    "    dy = dy.flatten(1)\n",
    "    dx = dx.flatten(1)\n",
    "    \n",
    "    d = 1 - (dx + np.roll(dx, s[0]) + dy + np.roll(dy, 1))\n",
    "\n",
    "    a = spdiags(np.vstack((dx, dy)), [-s[0], -1], k, k)\n",
    "    a = a + a.T + spdiags(d, 0, k, k)\n",
    "\n",
    "    temp = spsolve(a, image1).reshape(s[::-1])\n",
    "    \n",
    "    base = np.rollaxis(temp,1)\n",
    "    detail = image - base\n",
    "    return (base), (detail)\n",
    "\n",
    "\n",
    "st,dt = wlsfilter_layer (imaget[:,:,0]) # 0 --> lightness layer only, 1 -> a, 2 -> b\n",
    "sr,dr = wlsfilter_layer (imager[:,:,0])\n",
    "\n",
    "cv2.imshow('img',sr)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() \n",
    "\n",
    "# io.imshow(st)\n",
    "# io.show()\n",
    "# io.imshow(dt,cmap='gray')\n",
    "# io.show()\n",
    "\n",
    "# io.imshow(sr)\n",
    "# io.show()\n",
    "# io.imshow(dr,cmap='gray')\n",
    "# io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PointInsideTriangle2(pt,poly):\n",
    "    hull = cv2.convexHull(np.array(poly))\n",
    "    dist = cv2.pointPolygonTest(hull,(pt[0], pt[1]),False)\n",
    "    if dist>=0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpartition(landmarks, size0, size1):\n",
    "\n",
    "    cmat = np.zeros((size0,size1))\n",
    "\n",
    "    left_eye = landmarks[36:41] \n",
    "    right_eye = landmarks[42:47]\n",
    "    mouth = landmarks[48:67]\n",
    "    lips = landmarks[48:54] + landmarks[54:60] + landmarks[61:63] + landmarks[65:67]\n",
    "    skin = landmarks[0:26]\n",
    "\n",
    "    for y in range(size0):\n",
    "        for x in range(size1):\n",
    "            if PointInsideTriangle2((x,y),skin):\n",
    "                if PointInsideTriangle2((x,y),left_eye):\n",
    "                    cmat[y][x] = 3\n",
    "                elif PointInsideTriangle2((x,y),right_eye):\n",
    "                    cmat[y][x] = 3\n",
    "                elif PointInsideTriangle2((x,y),lips):\n",
    "                    if PointInsideTriangle2((x,y),mouth):\n",
    "                        cmat[y][x] = 3\n",
    "                    else:\n",
    "                        cmat[y][x] = 2\n",
    "                else:\n",
    "                    cmat[y][x] = 1\n",
    "\n",
    "    return cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f540377828>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = io.imread('bastf.jpg', 1)\n",
    "cmatrix = cpartition(landmarks_listref,img.shape[0],img.shape[1])\n",
    "io.imshow(cmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shading_transfer(Is, Es, cmat):\n",
    "    del_Es = cv2.Laplacian(Es,cv2.CV_64F)\n",
    "    del_Is = cv2.Laplacian(Is,cv2.CV_64F)\n",
    "    del_Rs = del_Is.copy()\n",
    "    \n",
    "    for y in range(Is.shape[0]):\n",
    "        for x in range(Is.shape[1]):\n",
    "            if(cmat[y][x]==1):\n",
    "                beta = 1\n",
    "            else:\n",
    "                beta = 0\n",
    "            if( beta * abs(del_Es[y][x]) > abs(del_Is[y][x])):\n",
    "                del_Rs[y][x] = del_Es[y][x]\n",
    "                \n",
    "    return del_Rs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lap = shading_transfer(st,sr,cmatrix)\n",
    "resulty = lap - st\n",
    "\n",
    "resultd = dt + 0.5*dr\n",
    "\n",
    "cv2.imshow('img',resultd)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input, img_example\n",
    "\n",
    "# 1\n",
    "img_input, img_example = rgb2lab(imgr)\n",
    "\n",
    "# 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mask(mask):\n",
    "    result=mask\n",
    "    for i in range(mask.shape[0]):\n",
    "        for j in range(mask.shape[1]):\n",
    "            if mask[i][j] != 0:\n",
    "                result[i][j] = 1\n",
    "    mask = result\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgwer = cv2.imread('T.jpg', 1)\n",
    "imgwet = cv2.imread('R.jpg', 1)\n",
    "\n",
    "imager = rgb2lab(imgr)\n",
    "imaget = rgb2lab(imgt)\n",
    "\n",
    "mask = prepare_mask(imager[:,:,0])\n",
    "\n",
    "cv2.imshow('img',mask)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend(img_target, img_source, img_mask, offset=(0, 0)):\n",
    "    # compute regions to be blended\n",
    "    region_source = (\n",
    "            max(-offset[0], 0),\n",
    "            max(-offset[1], 0),\n",
    "            min(img_target.shape[0]-offset[0], img_source.shape[0]),\n",
    "            min(img_target.shape[1]-offset[1], img_source.shape[1]))\n",
    "    region_target = (\n",
    "            max(offset[0], 0),\n",
    "            max(offset[1], 0),\n",
    "            min(img_target.shape[0], img_source.shape[0]+offset[0]),\n",
    "            min(img_target.shape[1], img_source.shape[1]+offset[1]))\n",
    "    region_size = (region_source[2]-region_source[0], region_source[3]-region_source[1])\n",
    "\n",
    "    # clip and normalize mask image\n",
    "    img_mask = img_mask[region_source[0]:region_source[2], region_source[1]:region_source[3]]\n",
    "    img_mask = prepare_mask(img_mask)\n",
    "    img_mask[img_mask==0] = False\n",
    "    img_mask[img_mask!=False] = True\n",
    "\n",
    "    # create coefficient matrix\n",
    "    A = spdiags.identity(np.prod(region_size), format='lil')\n",
    "    for y in range(region_size[0]):\n",
    "        for x in range(region_size[1]):\n",
    "            if img_mask[y,x]:\n",
    "                index = x+y*region_size[1]\n",
    "                A[index, index] = 4\n",
    "                if index+1 < np.prod(region_size):\n",
    "                    A[index, index+1] = -1\n",
    "                if index-1 >= 0:\n",
    "                    A[index, index-1] = -1\n",
    "                if index+region_size[1] < np.prod(region_size):\n",
    "                    A[index, index+region_size[1]] = -1\n",
    "                if index-region_size[1] >= 0:\n",
    "                    A[index, index-region_size[1]] = -1\n",
    "    A = A.tocsr()\n",
    "    \n",
    "    # create poisson matrix for b\n",
    "    P = pyamg.gallery.poisson(img_mask.shape)\n",
    "\n",
    "    # for each layer (ex. RGB)\n",
    "    for num_layer in range(img_target.shape[2]):\n",
    "        # get subimages\n",
    "        t = img_target[region_target[0]:region_target[2],region_target[1]:region_target[3],num_layer]\n",
    "        s = img_source[region_source[0]:region_source[2], region_source[1]:region_source[3],num_layer]\n",
    "        t = t.flatten()\n",
    "        s = s.flatten()\n",
    "\n",
    "        # create b\n",
    "        b = P * s\n",
    "        for y in range(region_size[0]):\n",
    "            for x in range(region_size[1]):\n",
    "                if not img_mask[y,x]:\n",
    "                    index = x+y*region_size[1]\n",
    "                    b[index] = t[index]\n",
    "\n",
    "        # solve Ax = b\n",
    "        x = pyamg.solve(A,b,verb=False,tol=1e-10)\n",
    "\n",
    "        # assign x to target image\n",
    "        x = np.reshape(x, region_size)\n",
    "        x[x>255] = 255\n",
    "        x[x<0] = 0\n",
    "        x = np.array(x, img_target.dtype)\n",
    "        img_target[region_target[0]:region_target[2],region_target[1]:region_target[3],num_layer] = x\n",
    "\n",
    "    return img_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import menpo.shape as shape\n",
    "import menpo.image as im\n",
    "\n",
    "re= cv2.imread('70289503_2447351508705408_6307667250979536896_o.jpg');\n",
    "ro = cv2.imread('uptown-girl-model-2.jpg', 1)\n",
    "dst=[[],[]]\n",
    "\n",
    "source = np.asarray(landmarks_listref)\n",
    "\n",
    "target = np.asarray(landmarks_list)\n",
    "\n",
    "# print(source)\n",
    "\n",
    "# obj = mio.ThinPlateSplines(  )\n",
    "# src =  shape.PointCloud(source)\n",
    "# trg =  shape.PointCloud(target)\n",
    "\n",
    "# imaaagee = mio.ThinPlateSplines(src,trg)\n",
    "# pc = imaaagee.aligned_source()\n",
    "# pv = np.asarray(pc)\n",
    "\n",
    "# pv =  shape.PointCloud(pc)\n",
    "# pv = np.asarray(pv)\n",
    "\n",
    "# cv2.imshow('img',pv)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "# obj = im.BooleanImage(target)\n",
    "# obj.warp_to_shape(src,warp_landmarks = True)\n",
    "\n",
    "# ob = cv2.ThinPlateSplineShapeTransformer(landmarks_listref,landmarks_list)\n",
    "\n",
    "# output = cv2.ShapeTransformer.warpImage(re, dst)\n",
    "# cv2.getAffineTransform()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cc,h = alignImages(re,ro)\n",
    "cv2.imshow('img',cc)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re= cv2.imread('70289503_2447351508705408_6307667250979536896_o.jpg');\n",
    "ro = cv2.imread('Angelina-Jolie.jpg.webp', 1)\n",
    "dst=[[],[]]\n",
    "\n",
    "source = np.asarray(landmarks_listref)\n",
    "\n",
    "target = np.asarray(landmarks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import morphops as mops\n",
    "import thinplate as tps\n",
    "\n",
    "\n",
    "def show_warped(img, warped):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16,8))\n",
    "    axs[0].axis('off')\n",
    "    axs[1].axis('off')\n",
    "    axs[0].imshow(img[...,::-1], origin='upper')\n",
    "    axs[0].scatter(c_src[:, 0]*img.shape[1], c_src[:, 1]*img.shape[0], marker='+', color='black')\n",
    "    axs[1].imshow(warped[...,::-1], origin='upper')\n",
    "    axs[1].scatter(c_dst[:, 0]*warped.shape[1], c_dst[:, 1]*warped.shape[0], marker='+', color='black')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def warp_image_cv(img, c_src, c_dst, dshape=None):\n",
    "    dshape = dshape or img.shape\n",
    "    theta = tps.tps_theta_from_points(c_src, c_dst, reduced=True)\n",
    "    grid = tps.tps_grid(theta, c_dst, dshape)\n",
    "    mapx, mapy = tps.tps_grid_to_remap(grid, img.shape)\n",
    "    return cv2.remap(img, mapx, mapy, cv2.INTER_CUBIC)\n",
    "\n",
    "warped = warp_image_cv(ro,source,target)\n",
    "show_warped(warped,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pylab as pyp\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Anisotropic diffusion.\n",
    "    \n",
    "    Cian Conway - 10126767\n",
    "    Patrick Stapleton - 10122834\n",
    "    Ivan McCaffrey - 10098119\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "resultImage = io.imread('70289503_2447351508705408_6307667250979536896_o.jpg')  ## Specify the original file path\n",
    "\n",
    "im_min, im_max = resultImage.min(), resultImage.max()\n",
    "\n",
    "print (\"Original image:\", resultImage.shape, resultImage.dtype, im_min, im_max)\n",
    "resultImage = (resultImage - im_min) / (float)(im_max - im_min)   ## Conversion\n",
    "print (\"Perona-Malik Anisotropic Diffusion:\", resultImage.shape, resultImage.dtype, resultImage.min(), resultImage.max())\n",
    "\n",
    "pyp.figure('Image BEFORE Perona-Malik anisotropic diffusion')\n",
    "pyp.imshow(resultImage, cmap='gray')\n",
    "pyp.axis('on')\n",
    "\n",
    "\n",
    "pyp.show() #Display image\n",
    "\n",
    "\"\"\"\n",
    "    ****Stopping Functions*****\n",
    "    Alternate Stopping Function\n",
    "    def f(lam,b): \n",
    "        func = 1/(1 + ((lam/b)**2))\n",
    "        return func\n",
    "    \n",
    "\"\"\"\n",
    "def f(lam,b):\n",
    "    return np.exp(-1* (np.power(lam,2))/(np.power(b,2)))\n",
    "\n",
    "def anisodiff(im, steps, b, lam = 0.25):  #takes image input, the number of iterations, \n",
    "    \n",
    "\n",
    "    im_new = np.zeros(im.shape, dtype=im.dtype) \n",
    "    for t in range(steps): \n",
    "        dn = im[:-2,1:-1] - im[1:-1,1:-1] \n",
    "        ds = im[2:,1:-1] - im[1:-1,1:-1] \n",
    "        de = im[1:-1,2:] - im[1:-1,1:-1] \n",
    "        dw = im[1:-1,:-2] - im[1:-1,1:-1] \n",
    "        im_new[1:-1,1:-1] = im[1:-1,1:-1] +\\\n",
    "                            lam * (f(dn,b)*dn + f (ds,b)*ds + \n",
    "                                    f (de,b)*de + f (dw,b)*dw) \n",
    "        im = im_new \n",
    "    return im\n",
    "  \n",
    "\n",
    "im2 = anisodiff(resultImage, 60, 0.15, 0.15)\n",
    "pyp.figure('Image AFTER Perona-Malik anisotropic diffusion')\n",
    "pyp.imshow(im2, cmap='gray')\n",
    "pyp.axis('on')\n",
    "\n",
    "\n",
    "pyp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selem = rectangle(4,5)\n",
    "gray_img = rgb2gray(im2)\n",
    "\n",
    "# im1 = image.filter(ImageFilter.BLUR)\n",
    "# blurred_face = ndimage.gaussian_filter(gray_img, sigma=3)\n",
    "\n",
    "io.imshow(im2)\n",
    "io.show()\n",
    "\n",
    "for j in range(gray_img.shape[1]-1):\n",
    "        for i in range(gray_img.shape[0]-1):\n",
    "            if(im2[i,j] < 1):\n",
    "                im2[i,j]=0\n",
    "                \n",
    "io.imshow(im2)\n",
    "io.show()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = io.imread('79111184_1304227093298684_6593981148299264000_n.jpg')\n",
    "# print(img.shape[0],img.shape[1])\n",
    "# D=10\n",
    "# h=img.shape[0]\n",
    "# w=img.shape[1]\n",
    "\n",
    "# def detect_faces(cascade, test_image, scaleFactor = 1.1):\n",
    "#     # create a copy of the image to prevent any changes to the original one.\n",
    "#     image_copy = test_image.copy()\n",
    "\n",
    "#     #convert the test image to gray scale as opencv face detector expects gray images\n",
    "#     gray_image = cv2.cvtColor(image_copy, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     # Applying the haar classifier to detect faces\n",
    "#     faces_rect = cascade.detectMultiScale(gray_image, scaleFactor=scaleFactor, minNeighbors=5)\n",
    "\n",
    "#     for (x, y, w, h) in faces_rect:\n",
    "#         cv2.rectangle(image_copy, (x, y), (x+w, y+h), (0, 255, 0), 15)\n",
    "\n",
    "#     return image_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_image = cv2.imread('79111184_1304227093298684_6593981148299264000_n.jpg')\n",
    "\n",
    "# # Convert color image to grayscale for Viola-Jones\n",
    "# grayscale_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# face_cascade = cv2.CascadeClassifier('path/to/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# detected_faces = face_cascade.detectMultiScale(grayscale_image)\n",
    "\n",
    "# for (column, row, width, height) in detected_faces:\n",
    "#     cv.rectangle(\n",
    "#         original_image,\n",
    "#         (column, row),\n",
    "#         (column + width, row + height),\n",
    "#         (0, 255, 0),\n",
    "#         2\n",
    "#     )\n",
    "    \n",
    "# cv.imshow('Image', original_image)\n",
    "# cv.waitKey(0)\n",
    "# cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  #loading image\n",
    "# haar_face_cascade = cv2.CascadeClassifier('data/haarcascade/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# test_image = cv2.imread('79111184_1304227093298684_6593981148299264000_n.jpg')\n",
    "\n",
    "#   # Converting to grayscale\n",
    "# test_image_gray = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#   # Displaying grayscale image\n",
    "# plt.imshow(test_image_gray, cmap='gray')\n",
    "\n",
    "# #call the function to detect faces\n",
    "# faces = detect_faces(haar_face_cascade, test_image2)\n",
    "\n",
    "#  #convert to RGB and display image\n",
    "# plt.imshow(convertToRGB(faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
